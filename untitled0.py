# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sn003LHGzk2rxK5XFAV2eNMUhS8Tufvs
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import json
import librosa
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Define paths
DATASET_PATH = "/content/drive/MyDrive/age"  # Path to your dataset folder
JSON_PATH = "data2.json"  # Path to save the extracted MFCCs and metadata
SAMPLES_TO_CONSIDER = 16000  # 1 second of audio at 16 kHz sample rate

def preprocess_dataset(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512):
    """
    Extracts MFCCs from the audio dataset and saves them to a JSON file.
    :param dataset_path (str): Path to dataset containing age and gender folders
    :param json_path (str): Path to JSON file to save MFCCs and metadata
    :param num_mfcc (int): Number of MFCCs to extract
    :param n_fft (int): Number of samples for FFT
    :param hop_length (int): Number of samples between each FFT window
    """
    # Initialize dictionary to store data
    data = {
        "mapping": [],       # List of all age-gender combinations (labels)
        "labels": [],        # Encoded labels for each file
        "MFCCs": [],         # MFCC features for each file
        "files": [],         # File paths for each file
        "original_labels": []  # Original label (e.g., "female 50+ Senior")
    }

    # Track the total number of samples processed
    total_samples = 0

    # Loop through all age group and gender subfolders
    for root, dirs, files in os.walk(dataset_path):
        # Sort directories and files to maintain a consistent label order
        dirs.sort()
        files.sort()

        # Extract age and gender from directory structure
        # Expected structure: /age/age_group/gender/filename.wav
        path_parts = root.split('/')
        if len(path_parts) >= 4:
            gender = path_parts[-2]   # Gender (e.g., "female" or "male")
            age_group = path_parts[-3]  # Age group (e.g., "50+ Senior")
            label = f"{gender} {age_group}"  # Combine gender and age group for unique label

            # Add label to mapping if not already present
            if label not in data["mapping"]:
                data["mapping"].append(label)
            label_index = data["mapping"].index(label)

            print(f"\nProcessing folder: {label}")

            # Process all audio files in the current folder
            samples = 0
            for filename in files:
                file_path = os.path.join(root, filename)

                # Load the audio file
                signal, sample_rate = librosa.load(file_path, sr=16000)

                # Ensure the audio has at least SAMPLES_TO_CONSIDER samples
                if len(signal) >= SAMPLES_TO_CONSIDER:
                    # Trim the signal to SAMPLES_TO_CONSIDER length
                    signal = signal[:SAMPLES_TO_CONSIDER]

                    # Extract MFCCs
                    MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)

                    # Store extracted features and metadata
                    data["MFCCs"].append(MFCCs.T.tolist())
                    data["labels"].append(label_index)
                    data["files"].append(file_path)
                    data["original_labels"].append(label)

                    # Print formatted output
                    print(f"Processed file: {file_path}, Label: {label}")

                    # Increment counters
                    samples += 1
                    total_samples += 1
                else:
                    print(f"File {file_path} is too short, skipping.")

            print(f"Processed {samples} samples in folder '{label}'")

    print(f"\nTotal samples processed: {total_samples}")

    # Save data to JSON file
    with open(json_path, "w") as fp:
        json.dump(data, fp, indent=4)
    print(f"Data saved to {json_path}")

# Run the preprocessing function
if __name__ == "__main__":
    preprocess_dataset(DATASET_PATH, JSON_PATH)

import json
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from datetime import datetime

# Logging directory for TensorBoard
logdir = "logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

# Constants
DATA_PATH = "data2.json"
SAVED_MODEL_PATH = "model02.keras"
EPOCHS = 60
BATCH_SIZE = 10
PATIENCE = 5
LEARNING_RATE = 0.0001

# Load data from JSON
def load_data(data_path):
    with open(data_path, "r") as fp:
        data = json.load(fp)
    X = np.array(data["MFCCs"])
    y = np.array(data["labels"])
    print("Training sets loaded!")
    return X, y

# Prepare dataset with train/validation/test split
def prepare_dataset(data_path, test_size=0.3, validation_size=0.1):
    X, y = load_data(data_path)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
    X_train, X_validation, y_train, y_validation = train_test_split(
        X_train, y_train, test_size=validation_size
    )

    # Add channel dimension
    X_train = X_train[..., np.newaxis]
    X_test = X_test[..., np.newaxis]
    X_validation = X_validation[..., np.newaxis]

    return X_train, y_train, X_validation, y_validation, X_test, y_test

# Build the convolutional neural network model
def build_model(input_shape, loss="sparse_categorical_crossentropy", learning_rate=0.0001):
    model = tf.keras.models.Sequential()

    # 1st convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape,
                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))

    # 2nd convolutional layer
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',
                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))

    # 3rd convolutional layer
    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu',
                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))

    # Flatten and dense layers
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.3))

    # Output layer
    model.add(tf.keras.layers.Dense(455, activation='softmax'))

    # Compile model
    optimiser = tf.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimiser, loss=loss, metrics=["accuracy"])
    model.summary()
    return model

# Train the model
def train(model, epochs, batch_size, patience, X_train, y_train, X_validation, y_validation):
    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor="accuracy", min_delta=0.001, patience=patience)
    history = model.fit(
        X_train,
        y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_validation, y_validation),
        callbacks=[tensorboard_callback]
    )
    return history

# Plot training history
def plot_history(history):
    fig, axs = plt.subplots(2, figsize=(10, 8))
    # Accuracy subplot
    axs[0].plot(history.history["accuracy"], label="Training Accuracy", marker='o', linestyle='-', color='b')
    axs[0].plot(history.history['val_accuracy'], label="Validation Accuracy", marker='o', linestyle='--', color='red')
    axs[0].set_ylabel("Accuracy", fontsize=16)
    axs[0].set_xlabel("Epoch", fontsize=16)
    axs[0].legend(loc="lower right")
    axs[0].set_title("Accuracy Evaluation", fontsize=18)  # Increased title font size
    axs[0].tick_params(axis='both', which='major', labelsize=14)  # Increased tick label font size


    # Loss subplot
    axs[1].plot(history.history["loss"], label="Training Loss", marker='o', linestyle='-', color='blue')
    axs[1].plot(history.history['val_loss'], label="Validation Loss", marker='o', linestyle='--', color='red')
    axs[1].set_ylabel("Loss",fontsize=16)
    axs[1].set_xlabel("Epoch",fontsize=16)
    axs[1].legend(loc="upper right")
    axs[1].set_title("Loss Evaluation")
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.tight_layout()
    plt.savefig("mrelu.png",dpi=400, bbox_inches='tight')
    plt.show()
# Main function
def main():
    # Prepare dataset
    X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)

    # Create model
    input_shape = (X_train.shape[1], X_train.shape[2], 1)
    model = build_model(input_shape, learning_rate=LEARNING_RATE)

    # Train model
    history = train(model, EPOCHS, BATCH_SIZE, PATIENCE, X_train, y_train, X_validation, y_validation)

    # Plot history
    plot_history(history)

    # Evaluate model on test set
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print("\nTest loss: {}, Test accuracy: {:.2f}%".format(test_loss, 100 * test_acc))

    # Save model
    model.save(SAVED_MODEL_PATH)

# Run script
if __name__ == "__main__":
    main()



!pip install ann_visualizer

import json
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from datetime import datetime

# Constants
logdir = "logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
DATA_PATH = "data2.json"
SAVED_MODEL_PATH = "model02.keras"
EPOCHS = 60
BATCH_SIZE = 10
PATIENCE = 5
LEARNING_RATE = 0.0001

# Load data
def load_data(data_path):
    with open(data_path, "r") as fp:
        data = json.load(fp)
    X = np.array(data["MFCCs"])
    y = np.array(data["labels"])
    print("Training sets loaded!")
    return X, y

# Prepare dataset
def prepare_dataset(data_path, test_size=0.3, validation_size=0.1):
    X, y = load_data(data_path)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
    X_train, X_validation, y_train, y_validation = train_test_split(
        X_train, y_train, test_size=validation_size
    )
    X_train = X_train[..., np.newaxis]
    X_test = X_test[..., np.newaxis]
    X_validation = X_validation[..., np.newaxis]
    return X_train, y_train, X_validation, y_validation, X_test, y_test

# Build model
def build_model(input_shape, loss="sparse_categorical_crossentropy", learning_rate=0.0001):
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape,
                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))

    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',
                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))

    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu',
                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))

    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.3))
    model.add(tf.keras.layers.Dense(455, activation='softmax'))

    optimiser = tf.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimiser, loss=loss, metrics=["accuracy"])
    model.summary()
    return model

# Train model
def train(model, epochs, batch_size, patience, X_train, y_train, X_validation, y_validation):
    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor="accuracy", min_delta=0.001, patience=patience)
    history = model.fit(
        X_train,
        y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_validation, y_validation),
        callbacks=[tensorboard_callback]
    )
    return history

# Plot training history
def plot_history(history):
    fig, axs = plt.subplots(2, figsize=(10, 8))

    # Accuracy subplot
    axs[0].plot(history.history["accuracy"], label="Training Accuracy", marker='o', linestyle='-', color='b')
    axs[0].plot(history.history['val_accuracy'], label="Validation Accuracy", marker='o', linestyle='--', color='red')
    axs[0].set_ylabel("Accuracy")
    axs[0].set_xlabel("Epoch")
    axs[0].legend(loc="lower right")
    axs[0].set_title("Accuracy Evaluation")
    axs[1].set_ylabel("Accuracy",fontsize=16)
    axs[1].set_xlabel("Epoch",fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)

    # Loss subplot
    axs[1].plot(history.history["loss"], label="Training Loss", marker='o', linestyle='-', color='b')
    axs[1].plot(history.history['val_loss'], label="Validation Loss", marker='o', linestyle='--', color='red')
    axs[1].set_ylabel("Loss",fontsize=16)
    axs[1].set_xlabel("Epoch",fontsize=16)
    axs[1].legend(loc="upper right")
    axs[1].set_title("Loss Evaluation")
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.tight_layout()
    plt.savefig("history_with_x_marks.png",dpi=400,bbox_inches='tight')
    plt.show()

# Main function
def main():
    X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)
    input_shape = (X_train.shape[1], X_train.shape[2], 1)
    model = build_model(input_shape, learning_rate=LEARNING_RATE)
    history = train(model, EPOCHS, BATCH_SIZE, PATIENCE, X_train, y_train, X_validation, y_validation)
    plot_history(history)
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print("\nTest loss: {}, Test accuracy: {:.2f}%".format(test_loss, 100 * test_acc))
    model.save(SAVED_MODEL_PATH)

# Run script
if __name__ == "__main__":
    main()







# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

import os
import json
import numpy as np
import librosa
import tensorflow as tf
from sklearn.metrics import accuracy_score

# Paths
MODEL_PATH = "model02.keras"  # Path to the saved model
TEST_FOLDER_PATH = "/content/drive/MyDrive/agetest"  # Path to the test folder
SAMPLES_TO_CONSIDER = 16000  # 1 second of audio at 16 kHz sample rate

# Load the trained model
model = tf.keras.models.load_model(MODEL_PATH)
print("Model loaded successfully!")

# Load the class mapping
DATA_PATH = "data2.json"
with open(DATA_PATH, "r") as fp:
    data = json.load(fp)
    class_mapping = data["mapping"]  # Class labels

def preprocess_audio(file_path):
    """
    Preprocess a single audio file for prediction.
    :param file_path: Path to the audio file
    :return: Preprocessed MFCCs
    """
    signal, sample_rate = librosa.load(file_path, sr=16000)

    # Ensure the audio has at least SAMPLES_TO_CONSIDER samples
    if len(signal) < SAMPLES_TO_CONSIDER:
        print(f"File {file_path} is too short, skipping.")
        return None

    # Trim or pad the signal to SAMPLES_TO_CONSIDER length
    signal = signal[:SAMPLES_TO_CONSIDER]

    # Extract MFCCs
    MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=13, n_fft=2048, hop_length=512)

    # Add a new axis to match the model's input shape
    MFCCs = MFCCs.T[np.newaxis, ..., np.newaxis]
    return MFCCs

def predict_label(file_path):
    """
    Predict the label of an audio file.
    :param file_path: Path to the audio file
    :return: Predicted label
    """
    MFCCs = preprocess_audio(file_path)
    if MFCCs is None:
        return None

    # Predict the label
    predictions = model.predict(MFCCs)
    predicted_index = np.argmax(predictions)
    predicted_label = class_mapping[predicted_index]
    return predicted_label

# Evaluate the model on the test dataset
true_labels = []
predicted_labels = []

print("\nPredictions:")
for root, _, files in os.walk(TEST_FOLDER_PATH):
    for file in sorted(files):  # Sort files for consistent output
        file_path = os.path.join(root, file)

        # Derive the ground truth label from folder structure
        path_parts = root.split('/')
        if len(path_parts) >= 2:
            # Assuming folder structure: /age category/gender
            ground_truth_label = f"{path_parts[-2]} {path_parts[-3]}"
        else:
            print(f"Invalid folder structure for file: {file_path}")
            continue

        # Predict the label for the current file
        predicted_label = predict_label(file_path)

        if predicted_label is not None:
            # Append ground truth and predicted labels
            true_labels.append(ground_truth_label)
            predicted_labels.append(predicted_label)
            print(f"Processed file: {file_path}, True Label: {ground_truth_label}, Predicted Label: {predicted_label}")

# Calculate accuracy
accuracy = accuracy_score(true_labels, predicted_labels)
print(f"\nAccuracy on the test dataset: {accuracy * 100:.2f}%")





from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Generate the confusion matrix with all labels
cm = confusion_matrix(true_labels, predicted_labels, labels=class_mapping)

# Define the labels to exclude from the visualization
exclude_labels = ["MyDrive drive", "age MyDrive"]

# Create a mask for labels to include
included_labels = [label for label in class_mapping if label not in exclude_labels]

# Find indices of included labels
included_indices = [i for i, label in enumerate(class_mapping) if label in included_labels]

# Filter the confusion matrix and labels for plotting
filtered_cm = cm[np.ix_(included_indices, included_indices)]
filtered_labels = included_labels

# Plot the filtered confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(
    filtered_cm,
    annot=True,
    fmt="d",
    xticklabels=filtered_labels,
    yticklabels=filtered_labels,
    cmap="coolwarm",  # Choose a pleasant color palette
    annot_kws={"size": 14},  # Increase annotation font size
    cbar_kws={"shrink": 0.8}  # Adjust the size of the color bar
)

# Customize font sizes and layout
plt.xlabel("Predicted Label", fontsize=18)
plt.ylabel("True Label", fontsize=18)
plt.title("Confusion Matrix", fontsize=20)
plt.xticks(fontsize=14, rotation=45, ha='right')  # Rotate and align x-axis labels
plt.yticks(fontsize=14)

# Save the plot with high resolution and display it
plt.savefig("mrelu_confusion_matrix.png", dpi=400, bbox_inches='tight')
plt.show()



